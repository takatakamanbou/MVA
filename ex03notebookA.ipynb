{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOimpW2tMclsbfMClPrXN0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/MVA/blob/2023/ex03notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MVA2023 ex03notebookA\n",
        "\n",
        "<img width=64 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/MVA/MVA-logo.png\"> https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA/2023"
      ],
      "metadata": {
        "id": "P4preLw7SzqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## 重回帰分析 (1)\n",
        "----"
      ],
      "metadata": {
        "id": "Kjddq7j6S48o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これまでの回帰分析では，ゴリゴリ君の例（「気温」対「アイス売上数」）のように説明変数も被説明変数も1つの変数だけでできている場合を考えていました．\n",
        "ここでは，説明変数が2つ以上の変数でできている場合の回帰分析について考えます．\n",
        "\n",
        "どちらも **回帰分析** (regression analysis) ですが，区別をする場合は，前者を **単回帰分析**(simple regression analysis)，後者を **重回帰分析**(multiple regression analysis) と呼びます．\n"
      ],
      "metadata": {
        "id": "hHiVhy3jHUm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下，コードセルを上から順に実行しながら読んでいってね．"
      ],
      "metadata": {
        "id": "60pN0nMrzBRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()"
      ],
      "metadata": {
        "id": "cctqDki5y55F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### やりたいこと\n",
        "\n",
        "具体的なデータを例にあげて，重回帰分析でやりたいことを説明します．"
      ],
      "metadata": {
        "id": "DSO66qjtmXIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，データを Pandas の DataFrame として読み込みます．\n",
        "ここでは Pandas/DataFrame についての理解は求めませんので，このコードセルが何をやっているかはわからなくても構いません．\n",
        "\n",
        "ここで読み込んでいるデータは，\n",
        "\n",
        "「Pythonで理解する統計解析の基礎」 谷合廣紀，辻 真吾，技術評論社，2018.\n",
        "\n",
        "に掲載されているものです．以下の GitHub サイトで公開されています．\n",
        "https://github.com/ghmagazine/python_stat_sample"
      ],
      "metadata": {
        "id": "r0_ztvXbzeDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "df = pd.read_csv('https://github.com/ghmagazine/python_stat_sample/raw/master/data/ch12_scores_reg.csv')\n",
        "df.drop(columns='通学方法', inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "djVvht0JzJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このデータは，とある授業の受講者20名について，「小テスト」と「期末テスト」の点数，および各受講者の「期末テスト」前日の「睡眠時間」という3つのデータを集めたものとなっています（元のデータにはこれ以外に「通学方法」というのもありますが，ここでは省いています）．\n",
        "\n",
        "このようなデータを見たときに，「小テスト」と「睡眠時間」が「期末テスト」に影響していると考えるのは自然なことでしょう．\n",
        "単回帰分析を使ってその影響を調べるとしたら，次のような二通りのやり方が考えられます：\n",
        "\n",
        "- 「小テスト」を説明変数とし「期末テスト」を被説明変数とする\n",
        "- 「睡眠時間」を説明変数とし「期末テスト」を被説明変数とする\n",
        "\n",
        "以下のセルで実際にそのような単回帰分析をやってみています．"
      ],
      "metadata": {
        "id": "euU8_EhW0kb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 「小テスト」 vs 「期末テスト」の単回帰分析\n",
        "xvec = df['小テスト'].to_numpy()\n",
        "yvec = df['期末テスト'].to_numpy()\n",
        "X1 = np.vstack([xvec, np.ones_like(xvec)]).T\n",
        "#print(X1.shape, yvec.shape)\n",
        "rv = np.linalg.lstsq(X1, yvec, rcond=None)\n",
        "a1, b1 = rv[0]\n",
        "print(f'a1 = {a1:.3f}, b1 = {b1:.2f}')"
      ],
      "metadata": {
        "id": "6pJMnhPx5v4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 「睡眠時間」 vs 「期末テスト」の単回帰分析\n",
        "xvec = df['睡眠時間'].to_numpy()\n",
        "yvec = df['期末テスト'].to_numpy()\n",
        "X1 = np.vstack([xvec, np.ones_like(xvec)]).T\n",
        "#print(X1.shape, yvec.shape)\n",
        "rv = np.linalg.lstsq(X1, yvec, rcond=None)\n",
        "a2, b2 = rv[0]\n",
        "print(f'a2 = {a2:.3f}, b2 = {b2:.2f}')"
      ],
      "metadata": {
        "id": "kn1Y55Qs6c1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 二つの単回帰分析の結果を並べて可視化\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
        "ax[0].scatter(df['小テスト'], df['期末テスト'])\n",
        "xx = np.array([0, 10])\n",
        "ax[0].plot(xx, a1*xx + b1, color='red', label=f'$y = {a1:.3f} x + {b1:.2f}$')\n",
        "ax[0].set_xlim(0, 10)\n",
        "ax[0].set_ylim(0, 100)\n",
        "ax[0].set_xlabel('Quiz')\n",
        "ax[0].set_ylabel('Exam')\n",
        "ax[0].legend()\n",
        "ax[1].scatter(df['睡眠時間'], df['期末テスト'])\n",
        "ax[1].plot(xx, a2*xx + b2, color='red', label=f'$y = {a2:.3f} x + {b2:.2f}$')\n",
        "ax[1].set_xlim(0, 10)\n",
        "ax[1].set_ylim(0, 100)\n",
        "ax[1].set_xlabel('Sleep')\n",
        "ax[1].set_ylabel('Exam')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9c75dgMV6vvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このように，単回帰分析を使えば「小テスト」が「期末テスト」に及ぼす影響や「睡眠時間」が「期末テスト」に及ぼす影響を調べることはできます．\n",
        "しかし，説明変数は「小テスト」と「睡眠時間」のどちらか一方だけですので，両方あわせたものが「期末テスト」にどんな影響を及ぼすのかはよくわかりません．\n",
        "\n"
      ],
      "metadata": {
        "id": "uyu-N5St6pvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このような場合に重回帰分析を使うと，「小テスト」と「睡眠時間」の両方を説明変数とし「期末テスト」を被説明変数とした回帰分析を行うことができます．\n",
        "\n",
        "具体的には，次のような式を考えます．\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 \\qquad (*)\n",
        "$$\n",
        "ここで，$x_1, x_2, y$ はそれぞれ，「小テスト」，「睡眠時間」，「期末テスト」の値を表します．\n",
        "$w_0, w_1, w_2$ の値を適当に決めれば，$(x_1, x_2)$ から $y$ の値を求めることができます．\n",
        "したがって，単回帰分析の場合と同じように，与えられたデータにうまく当てはまるように $(w_0, w_1, w_2)$ の値を決めてやろう，という問題設定を考えることができます．"
      ],
      "metadata": {
        "id": "ymuoynpr1jPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ちゃんとした話（「うまく当てはまる」のちゃんとした定義とか）は後回しにして，とりあえず上記のデータの例でやってみましょう．まず，$(x_1, x_2, y)$ の散布図を描いてみるとこんなんです．"
      ],
      "metadata": {
        "id": "zQU2NaJmAwZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (小テスト, 睡眠時間, 期末テスト) の散布図\n",
        "fig = plt.figure(figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(df['小テスト'], df['睡眠時間'], df['期末テスト'], s=50, color='blue')\n",
        "ax.set_xlabel('$x_1$: Quiz')\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylabel('$x_2$: Sleep')\n",
        "ax.set_ylim(0, 10)\n",
        "ax.set_zlabel('$y$: Exam')\n",
        "ax.set_zlim(0, 100)\n",
        "ax.view_init(25, -70)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pXx48IgPBA_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "個々の青い点がひとりひとりの $(小テスト, 睡眠時間, 期末テスト)$　の値を表しています．\n",
        "これに対して，式$(*)$ はこの空間に広がる2次元平面を表します．\n",
        "$(w_0, w_1, w_2)$ の値をいろいろ変えると平面の向きや位置が変わりますが，（最小二乗法を用いて）データに最もよく当てはまる平面の式を求めて表示すると，次のようになります．"
      ],
      "metadata": {
        "id": "5G4Wm50FBmvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (小テスト, 睡眠時間) vs 期末テストの重回帰分析\n",
        "X = df.loc[:, ['小テスト', '睡眠時間']].to_numpy()\n",
        "Y = df['期末テスト'].to_numpy()\n",
        "X1 = np.vstack([np.ones(len(X)), X.T]).T\n",
        "rv = np.linalg.lstsq(X1, Y, rcond=None)\n",
        "w = rv[0]\n",
        "print(f'w_0 = {w[0]:.3f},   w_1 = {w[1]:.3f},   w_2 = {w[2]:.3f}')\n",
        "\n",
        "# 平面を可視化するためのデータの準備\n",
        "xmin, xmax = 0, 10\n",
        "ymin, ymax = 0, 10\n",
        "x_mesh, y_mesh = np.meshgrid(np.linspace(xmin, xmax, num=16), np.linspace(ymin, ymax, num=16))\n",
        "X1mesh = np.vstack((np.ones(len(x_mesh.ravel())), x_mesh.ravel(), y_mesh.ravel())).T\n",
        "z_mesh = (X1mesh @ w).reshape((x_mesh.shape[0], y_mesh.shape[1]))\n",
        "\n",
        "# 散布図と得られた平面を重ねて描画\n",
        "fig = plt.figure(figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X1[:, 1], X1[:, 2], Y, s=50, color='blue')\n",
        "ax.plot_wireframe(x_mesh, y_mesh, z_mesh, color='red',\n",
        "                  label=f'$y = {w[0]:.2f} + {w[1]:.2f} x_1 + {w[2]:.2f} x_2$')\n",
        "ax.set_xlabel('$x_1$: Quiz')\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylabel('$x_2$: Sleep')\n",
        "ax.set_ylim(0, 10)\n",
        "ax.set_zlabel('$y$: Exam')\n",
        "ax.set_zlim(0, 100)\n",
        "ax.view_init(25, -70)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rb3PtqcMC7fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "単回帰で得られた係数とこちらで得られた係数 $(w_0, w_1, w_2)$ の値は異なっていることに注意してください．\n",
        "\n",
        "このように式が得られれば，単回帰分析の場合と同じようにして，適当な $(x_1, x_2)$ の値に対する $y$ の値の予測値を算出することができますね．"
      ],
      "metadata": {
        "id": "zY8jUPSeEpUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (1, Quiz, Sleep) をならべた 9 x 3 行列\n",
        "XX = np.array([\n",
        "    [1, 2, 2], [1, 5, 2], [1, 8, 2],\n",
        "    [1, 2, 5], [1, 5, 5], [1, 8, 5],\n",
        "    [1, 2, 8], [1, 5, 8], [1, 8, 8],\n",
        "])\n",
        "# Exam の予測値\n",
        "YY = XX @ w\n",
        "# 表示\n",
        "for n in range(len(YY)):\n",
        "    print(f'小テスト {XX[n, 1]} 点，前日の睡眠時間 {XX[n, 2]} 時間のひとの期末テストの点数の予測値は {YY[n]:.1f} 点')"
      ],
      "metadata": {
        "id": "q-DlrejoFuKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 問題の定式化\n",
        "\n",
        "重回帰分析の問題をきちんと定式化しましょう（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: ここでは確率統計の言葉を用いた定式化をしていませんので，重回帰分析の問題の定式としては完全ではありません．その辺のことはまた後日解説します．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "7MDYZy1HHzoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$D$個の値 $x_1, x_2, \\ldots, x_D$ から一つの値 $y$ が予測できるとして，それらの間に\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + \\cdots + w_Dx_D\\qquad (1)\n",
        "$$\n",
        "\n",
        "という関係が成り立つと仮定します． $x_1, x_2, \\ldots, x_D$ を **説明変数**， $y$ を **被説明変数** といいます（注）．\n",
        "\n",
        "$(x_1, \\ldots, x_D)$ と $y$ の組 $N$ 個から成るデータ\n",
        "\n",
        "$$\n",
        "\\begin{array}{c}\n",
        "((x_{1,1}, x_{1,2}, \\ldots, x_{1,D}), y_1),\\\\\n",
        "((x_{2,1}, x_{2,2}, \\ldots, x_{2,D}), y_2),\\\\\n",
        "\\vdots \\\\\n",
        "((x_{N,1}, x_{N,2}, \\ldots, x_{N,D}), y_N)\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "が与えられたとき，式(1)がこれらのデータになるべく「良く当てはまる」ように $(D+1)$ 個のパラメータ $w_0, w_1, w_2, \\ldots, w_D$ の値を決めましょう．\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 説明変数を「独立変数」と呼ぶこともあります．被説明変数を「従属変数」または「目的変数」と呼ぶこともあります．\n",
        "</span>"
      ],
      "metadata": {
        "id": "kwY4Z1d1h9JX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これが重回帰分析の問題です．\n",
        "式(1)は，説明変数と被説明変数が作る $(D+1)$ 次元空間中の $D$次元平面を表す式です．上のデータの例では，$D=2$ で，$(小テスト, 睡眠時間, 期末テスト)$ の3次元空間中の2次元平面を表しています．\n",
        "\n",
        "重回帰分析の目的は，この平面がデータの点たちに「良く当てはまる」ようにパラメータを決めることです．ただし，この表現では「良く当てはまる」という部分がまだあいまいですので，ここをきちんと定めましょう．\n"
      ],
      "metadata": {
        "id": "1L-znhTAnYz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この問題において，$(x_{n,1}, x_{n,2}, \\ldots, x_{n,D})$ を式(1)に代入して得られる値\n",
        "\n",
        "$$\n",
        "\\hat{y}_n = w_0 + w_1x_{n,1} + w_2x_{n,2} + \\cdots + w_Dx_{n,D}\n",
        "$$\n",
        "\n",
        "は， $y$ の予測値といえます．\n",
        "一方，$(x_{n,1}, x_{n,2}, \\ldots, x_{n,D})$ に対応して実際に得られた $y$ の値は $y_n$ です．実際に得られた値 $y_n$ と予測値 $\\hat{y}_n$ とのずれを二乗誤差\n",
        "$\n",
        "(y_n - \\hat{y}_n)^2\n",
        "$\n",
        "で測ることにして，$N$個のデータについての二乗誤差の和（**残差平方和**）\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E(w_0, w_1, \\ldots, w_D) &= \\sum_{n=1}^{N}(y_n - \\hat{y}_n)^2 \\\\\n",
        "&= \\sum_{n=1}^{N}(y_n - (w_0 + w_1x_{n,1} + w_2x_{n,2} + \\cdots + w_Dx_{n,D}))^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "を考えてみます．\n",
        "予測値と実際の値とのずれが小さければ小さいほど，この値は小さくなりますから，この値は，データに対する式(1)の当てはまりの「良さ」の指標とみなせます（値が小さい方が良いので，より正確には「悪さ」の指標）．\n",
        "\n",
        "これで「良く当てはまる」の定義ができました．\n",
        "これを使って問題設定の最後の部分を書き直すと，次のようになります．\n"
      ],
      "metadata": {
        "id": "DQgmJqiDndud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "これらのデータに対する式(1)の当てはまりの良さを残差平方和\n",
        "\n",
        "$$\n",
        "E(w_0, w_1, \\ldots, w_D) = \\sum_{n=1}^{N}(y_n - (w_0 + w_1x_{n,1} + w_2x_{n,2} + \\cdots + w_Dx_{n,D}))^2 \\qquad (2)\n",
        "$$\n",
        "\n",
        "で測るものとして，これが最小となるように  $(D+1)$ 個のパラメータ $w_0, w_1, w_2, \\ldots, w_D$ の値を決めましょう．\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d9KbJi02t4ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，実際の値と予測値との二乗誤差の和を最小にする問題という形で定式化しました．\n",
        "これが最も一般的な重回帰分析ですが，当てはまりの良さの規準に二乗誤差とは異なるものを用いる場合もあります．特に区別したい場合は，ここで説明している方法を「**最小二乗法**による重回帰分析」と呼びます（注）．\n",
        "\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: データの性質や分析の目的によっては，二乗誤差とは異なる規準を用いた方が良い場合があります（発展的な内容になりますのでこれ以上の説明は省略します）．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "xpyDPm6-v2lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 最小二乗法の解\n",
        "\n",
        "最小二乗法による重回帰分析の問題の解は，単回帰の場合と同様の手順で求めることができます．\n",
        "ただし，文字が多くて式が複雑になるので，ベクトルを使って式を整理して考えることにします．"
      ],
      "metadata": {
        "id": "eYfpgw5my81o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，$n$番目のデータの値 $x_{n,1}, x_{n,2}, \\ldots, x_{n,D}$ をならべたものの先頭に $1$ を付け足した $(D+1)$ 次元ベクトルを\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_n = (1, x_{n,1}, x_{n,2}, \\ldots, x_{n,D}) \\qquad (n = 1, 2, \\ldots, N)\n",
        "$$\n",
        "\n",
        "と表すことにします．先頭に余分な $1$ が付いているため， $D$ 次元ではなく $(D+1)$ 次元になっていることに注意．\n",
        "\n",
        "これに対して，$(D+1)$ 個のパラメータ $w_0, w_1, \\ldots, w_D$ をならべた $(D+1)$ 次元ベクトルを\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = (w_0, w_{1}, w_{2}, \\ldots, w_{D})\n",
        "$$\n",
        "\n",
        "と表すことにすると， $\\hat{y}_n$ を簡単に $\\hat{y}_n = \\mathbf{w}\\cdot \\mathbf{x}_n$ と書くことができます（$\\mathbf{x}\\cdot\\mathbf{y}$ はベクトル $\\mathbf{x}$ とベクトル $\\mathbf{y}$ の内積）．\n",
        "最小化したい残差平方和は\n",
        "\n",
        "$$\n",
        "E(\\mathbf{w}) = \\sum_{n=1}^{N}(y_n - \\hat{y}_n)^2 = \\sum_{n=1}^{N}(y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)^2\n",
        "$$\n",
        "\n",
        "です．"
      ],
      "metadata": {
        "id": "adKF3AyP0mIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "準備ができました．\n",
        "われわれの目的は，$E(\\mathbf{w})$ を最小にする $\\mathbf{w}$ を求めることです．\n",
        "単回帰の解の導出過程と同じように，$E(\\mathbf{w})$ を $\\mathbf{w}$ の各要素で偏微分したものを $0$ とおいて，解が満たす方程式を求めます．\n",
        "$E(\\mathbf{w})$ の $w_d$ ($d=0,1,\\ldots,D$) での偏微分は次のように計算できます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial E(\\mathbf{w})}{\\partial w_d} &= 2\\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n) \\frac{\\partial}{\\partial w_d}(y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)   \\\\\n",
        "&= 2\\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)\\left( 0 -\\frac{\\partial}{\\partial w_d}(w_0+w_1x_{n,1}+\\cdots + w_Dx_{n,D})\\right) \\\\\n",
        "&= 2\\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)(-x_{n,d}) \\qquad (d = 0, 1, 2, \\ldots, D)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$\\mathbf{x}_n$ も $\\mathbf{w}$ も要素は添字の番号が $0$ のものから $D$ のものまである（$(D+1)$個ある）ことに注意してください．さらに，$\\mathbf{x}_n$ の $0$ 番目の要素 $x_{n,0}$ は常に $1$ です．\n",
        "\n",
        "$\\frac{\\partial E(\\mathbf{w})}{\\partial w_d} = 0$ とおくと， $(D+1)$ 個の未知数 $w_0, w_1, \\ldots, w_D$ に対して $(D+1)$ 個の式から成る次の連立方程式が得られます．\n",
        "\n",
        "$$\n",
        "\\left\\{\n",
        "    \\begin{aligned}\n",
        "    \\sum x_{n,0}\\mathbf{w}\\cdot\\mathbf{x}_n &= \\sum x_{n,0}y_n\\\\\n",
        "    \\sum x_{n,1}\\mathbf{w}\\cdot\\mathbf{x}_n &= \\sum x_{n,1}y_n\\\\\n",
        "    \\vdots \\\\\n",
        "    \\sum x_{n,D}\\mathbf{w}\\cdot\\mathbf{x}_n &= \\sum x_{n,D}y_n\\\\\n",
        "    \\end{aligned}\n",
        "\\right. \\qquad (3)\n",
        "$$\n",
        "\n",
        "簡単のため，和記号 $\\sum$ の上下の $n=1$ と $N$ を省略して書いてます．\n"
      ],
      "metadata": {
        "id": "7SX8xpUw30Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この連立方程式を $\\mathbf{w}$ について解けば残差平方和を最小にするパラメータ $\\mathbf{w}$ が求まりますが，このままでは見通しが悪いので，単回帰の場合と同様にデータの値をならべた行列を考えて式を整理することにします．"
      ],
      "metadata": {
        "id": "rvpVUXxg8bKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，ベクトル $\\mathbf{x}_n$ を行ベクトル（要素が行方向に並んだベクトル）とみなして，それらを列方向に並べた行列を $X$ とします．\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix}\n",
        "\\mathbf{x}_1\\\\\n",
        "\\mathbf{x}_2\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{x}_N\\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,D}\\\\\n",
        "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,D}\\\\\n",
        "& & \\vdots\\\\\n",
        "1 & x_{N,1} & x_{N,2} & \\cdots & x_{N,D}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$X$ は $N\\times(D+1)$ の行列です．\n",
        "\n",
        "次に，$y_1, y_2, \\ldots, y_N$ を列方向に並べた行列を $Y$ とします．\n",
        "\n",
        "$$\n",
        "Y = \\begin{pmatrix}\n",
        "y_1\\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$Y$ は $N\\times 1$ の行列です．"
      ],
      "metadata": {
        "id": "yxv9b1W_9LuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このとき，式(3)の連立方程式は，次のように書き直すことができます（注）．\n",
        "\n",
        "$$\n",
        "X^{\\top}X\n",
        "\\begin{pmatrix}\n",
        "w_0\\\\ w_1\\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= X^{\\top}Y \\qquad (4)\n",
        "$$\n",
        "\n",
        "この連立方程式は，単回帰分析と同様に **正規方程式** と呼ばれます．\n",
        "$D=1$ とすれば単回帰の正規方程式と等しいことが分かります（1を付け加えている場所が違うのでパラメータの並び順が違うだけ）．\n",
        "この正規方程式の解が，$E(\\mathbf{w})$ を最小にする $\\mathbf{w}$ となります．\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ggQ8YGqV_s6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "※注: 式変形の過程を少し省略しました．省略した部分を以下に書いておきます．\n",
        "式(3) の左辺の $d$ 番目（$d=0,1,\\ldots,D$）の行を取り出して考えると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sum_{n=1}^{N} x_{n,d}\\mathbf{w}\\cdot\\mathbf{x}_n &= \\sum_{n=1}^{N} (x_{n,d}\\mathbf{x}_n)\\cdot \\mathbf{w} = \\left( \\sum_{n=1}^{N} x_{n,d}\\mathbf{x}_n\\right) \\cdot\\mathbf{w} \\\\\n",
        "&= \\left( \\sum x_{n,d}x_{n,0}, \\sum x_{n,d}x_{n,1},  \\ldots, \\sum x_{n,d}x_{n,D} \\right) \\cdot \\mathbf{w}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "と書けるので，式(3)の左辺は\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\sum x_{n,0}x_{n,0} & \\sum x_{n,0}x_{n,1} & \\cdots & \\sum x_{n,0}x_{n,D} \\\\\n",
        "\\sum x_{n,1}x_{n,0} & \\sum x_{n,1}x_{n,1} & \\cdots & \\sum x_{n,1}x_{n,D} \\\\\n",
        "\\vdots & \\vdots & & \\vdots\\\\\n",
        "\\sum x_{n,D}x_{n,0} & \\sum x_{n,D}x_{n,1} & \\cdots & \\sum x_{n,D}x_{n,D} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= \\left(\\sum_{n=1}^{N} \\mathbf{x}_n^{\\top} \\mathbf{x}_n \\right)\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= X^{\\top}X\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "となります．\n",
        "\n",
        "式(3)の右辺の方はもっと簡単ですので，自分で導出してみてね．"
      ],
      "metadata": {
        "id": "gxFXuE47fWuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 決定係数\n",
        "\n"
      ],
      "metadata": {
        "id": "P9kzHjYfFP_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "回帰分析の結果の良し悪しを表す指標の一つに，**決定係数** というものがあります（「寄与率」と呼ぶこともあります）．\n",
        "**決定係数** は，「被説明変数の変動のうちどれくらいを説明変数によって説明できているか」を表す値です． 多くの場合，$r^2$ または $R^2$ という記号で表され，次のように定義されます．\n",
        "\n",
        "$$\n",
        "r^2 = 1 - \\frac{\\displaystyle\\sum_{n=1}^{N}(y_n - \\hat{y}_n)^2}{\\displaystyle\\sum_{n=1}^{N}(y_n - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "ただし，$\\hat{y}_n$ は上で説明している通り$(x_{n,1}, x_{n,2}, \\ldots, x_{n,D})$ に対応する予測値であり，$\\bar{y}$ は $y$ の平均です．\n",
        "\n",
        "分子の式\n",
        "\n",
        "$$\\sum_{n=1}^{N}(y_n - \\hat{y}_n)^2$$\n",
        "\n",
        "は，「予測残差の平方和」です．\n",
        "式(1)で $(x_{n,1}, x_{n,2}, \\ldots, x_{n,D})$ から $y$ を予測してみたが予測しきれず残った値の散らばり，を表します．\n",
        "一方，分母の式\n",
        "\n",
        "$$\\sum_{n=1}^{N}(y_n - \\bar{y})^2$$\n",
        "\n",
        "は，$y$ の分散を $N$ 倍したものになっており，「被説明変数$y$の変動の大きさ」，つまり，もともと $y$ の値がどれくらい散らばっていたか，を表します．つまり，\n",
        "\n",
        "$$\n",
        "r^2 = 1 - \\frac{\\mbox{予測しきれず残った値の散らばりの大きさ}}{\\mbox{もともとの$y$の散らばりの大きさ}}\n",
        "$$\n",
        "\n",
        "ということです．\n",
        "\n",
        "説明変数から被説明変数が完全に予測できた場合は，残差平方和が $0$ になりますので，$r^2 = 1$ となります．\n",
        "一方，予測が全くうまくいかない場合は，もともとの $y$ の散らばりが全部残ったままで分子の残差平方和が分母の値と等しくなるので，$r^2 = 0$ となります．\n",
        "したがって，$0 \\leq r^2 \\leq 1$ であり，$r^2$ の値は $1$ に近いほど当てはまりがよい，ということになります．\n",
        "\n",
        "しかし，実は，決定係数は，重回帰分析の結果の良し悪しを判断する指標としてはあまり良いものではありません．\n",
        "次回，その理由と，その点を改良した指標について説明します．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FBCOwslnEvlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下の図は，上記の説明を直感的に理解するための図です（ただし，重回帰分析ではなく単回帰分析の例です）．\n",
        "上の段が決定係数が大きい場合，下の段が決定係数が小さい場合で，\n",
        "左側が「もともとの $y$ の散らばりの大きさ」を，右側が「予測しきれず残った値の散らばりの大きさ」を表しています．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex11-residual.png\">\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex11-residual2.png\">"
      ],
      "metadata": {
        "id": "WVdb21fLKaw8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LIHAPGKJLKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}